"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[6143],{2793:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>_,contentTitle:()=>a,default:()=>l,frontMatter:()=>s,metadata:()=>i,toc:()=>p});var r=t(4848),o=t(8453);const s={title:"TensorRT-LLM \u90e8\u7f72 ChatGLM3",authors:"initencunter",tags:["docs","\u6559\u7a0b"]},a=void 0,i={id:"tutorial/TensorRT-llm/TensorRT-llm-ChatGLM3",title:"TensorRT-LLM \u90e8\u7f72 ChatGLM3",description:"\u5b89\u88c5 TensorRT-LLM",source:"@site/docs/tutorial/TensorRT-llm/TensorRT-llm-ChatGLM3.md",sourceDirName:"tutorial/TensorRT-llm",slug:"/tutorial/TensorRT-llm/TensorRT-llm-ChatGLM3",permalink:"/docs/tutorial/TensorRT-llm/TensorRT-llm-ChatGLM3",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tutorial/TensorRT-llm/TensorRT-llm-ChatGLM3.md",tags:[{inline:!0,label:"docs",permalink:"/docs/tags/docs"},{inline:!0,label:"\u6559\u7a0b",permalink:"/docs/tags/\u6559\u7a0b"}],version:"current",frontMatter:{title:"TensorRT-LLM \u90e8\u7f72 ChatGLM3",authors:"initencunter",tags:["docs","\u6559\u7a0b"]},sidebar:"tutorialSidebar",previous:{title:"Shamrock \u63a5\u5165 QQ",permalink:"/docs/tutorial/shamrock/"},next:{title:"koishi\u5bf9\u63a5mysql \uff08docker\uff09",permalink:"/docs/tutorial/mysql/"}},_={},p=[{value:"\u5b89\u88c5 TensorRT-LLM",id:"\u5b89\u88c5-tensorrt-llm",level:2},{value:"\u4e0b\u8f7d\u6a21\u578b",id:"\u4e0b\u8f7d\u6a21\u578b",level:2},{value:"\u8f6c\u6362\u6a21\u578b",id:"\u8f6c\u6362\u6a21\u578b",level:2},{value:"\u6784\u5efa\u5f15\u64ce",id:"\u6784\u5efa\u5f15\u64ce",level:2},{value:"\u5355\u6b21\u63a8\u7406",id:"\u5355\u6b21\u63a8\u7406",level:2},{value:"\u8fde\u7eed\u63a8\u7406",id:"\u8fde\u7eed\u63a8\u7406",level:2}];function u(n){const e={a:"a",code:"code",h2:"h2",p:"p",pre:"pre",...(0,o.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(e.h2,{id:"\u5b89\u88c5-tensorrt-llm",children:["\u5b89\u88c5 ",(0,r.jsx)(e.a,{href:"https://github.com/NVIDIA/TensorRT-LLM",children:"TensorRT-LLM"})]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.a,{href:"https://nvidia.github.io/TensorRT-LLM/installation/windows.html",children:"windows\u5b89\u88c5\u6559\u7a0b"})}),"\n",(0,r.jsx)(e.h2,{id:"\u4e0b\u8f7d\u6a21\u578b",children:"\u4e0b\u8f7d\u6a21\u578b"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-shell",children:"git clone git clone https://www.modelscope.cn/ZhipuAI/chatglm3-6b.git chatglm3_6b\n"})}),"\n",(0,r.jsx)(e.h2,{id:"\u8f6c\u6362\u6a21\u578b",children:"\u8f6c\u6362\u6a21\u578b"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-shell",children:"## \u5982\u679c\u63d0\u793a\u627e\u4e0d\u5230\u52a8\u6001\u94fe\u63a5\u5e93\uff0c\u53ef\u4ee5\u5230\u4ed3\u5e93\u4ee5\u5916\u7684\u76ee\u5f55\u6765\u8f6c\u6362\npython convert_checkpoint.py --model_dir chatglm3_6b --output_dir trt_ckpt/chatglm3_6b/fp16/1-gpu\n"})}),"\n",(0,r.jsx)(e.h2,{id:"\u6784\u5efa\u5f15\u64ce",children:"\u6784\u5efa\u5f15\u64ce"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-shell",children:"## \u5982\u679c\u63d0\u793a\u627e\u4e0d\u5230\u52a8\u6001\u94fe\u63a5\u5e93\uff0c\u53ef\u4ee5\u5230\u4ed3\u5e93\u4ee5\u5916\u7684\u76ee\u5f55\u6765\u6784\u5efa\ntrtllm-build --checkpoint_dir trt_ckpt/chatglm3_6b/fp16/1-gpu \\\n        --gemm_plugin float16 \\\n        --output_dir trt_engines/chatglm3_6b/fp16/1-gpu\n"})}),"\n",(0,r.jsx)(e.h2,{id:"\u5355\u6b21\u63a8\u7406",children:"\u5355\u6b21\u63a8\u7406"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-shell",children:'python ../run.py --input_text "\u4f60\u597d" --max_output_len 1000  --tokenizer_dir chatglm3_6b --engine_dir trt_engines/chatglm3_6b/fp16/1-gpu\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u8fde\u7eed\u63a8\u7406",children:"\u8fde\u7eed\u63a8\u7406"}),"\n",(0,r.jsxs)(e.p,{children:["\u5c06 ",(0,r.jsx)(e.code,{children:"../run.py"})," \u6539\u6210\u4e0b\u9762\u7684\u4ee3\u7801\uff0c\u7136\u540e\u8fd0\u884c"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-shell",children:"python ../run.py --max_output_len 1000  --tokenizer_dir chatglm3_6b --engine_dir trt_engines/chatglm3_6b/fp16/1-gpu\n"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"# SPDX-FileCopyrightText: Copyright (c) 2022-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport ast\nimport csv\nimport os\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nfrom utils import (DEFAULT_HF_MODEL_DIRS, DEFAULT_PROMPT_TEMPLATES,\n                   load_tokenizer, read_model_name, throttle_generator)\n\nimport tensorrt_llm\nimport tensorrt_llm.profiler\nfrom tensorrt_llm.logger import logger\nfrom tensorrt_llm.runtime import PYTHON_BINDINGS, ModelRunner\n\nif PYTHON_BINDINGS:\n    from tensorrt_llm.runtime import ModelRunnerCpp\n\n\ndef parse_arguments(args=None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--max_output_len', type=int, required=True)\n    parser.add_argument(\n        '--max_attention_window_size',\n        type=int,\n        default=None,\n        help=\n        'The attention window size that controls the sliding window attention / cyclic kv cache behavior'\n    )\n    parser.add_argument('--sink_token_length',\n                        type=int,\n                        default=None,\n                        help='The sink token length.')\n    parser.add_argument('--log_level', type=str, default='warning')\n    parser.add_argument('--engine_dir', type=str, default='engine_outputs')\n    parser.add_argument('--use_py_session',\n                        default=False,\n                        action='store_true',\n                        help=\"Whether or not to use Python runtime session\")\n    parser.add_argument(\n        '--input_text',\n        type=str,\n        nargs='+',\n        default=[\"Born in north-east France, Soyer trained as a\"])\n    parser.add_argument(\n        '--no_prompt_template',\n        dest='use_prompt_template',\n        default=True,\n        action='store_false',\n        help=\n        \"Whether or not to use default prompt template to wrap the input text.\")\n    parser.add_argument(\n        '--input_file',\n        type=str,\n        help=\n        'CSV or Numpy file containing tokenized input. Alternative to text input.',\n        default=None)\n    parser.add_argument('--max_input_length', type=int, default=923)\n    parser.add_argument('--output_csv',\n                        type=str,\n                        help='CSV file where the tokenized output is stored.',\n                        default=None)\n    parser.add_argument('--output_npy',\n                        type=str,\n                        help='Numpy file where the tokenized output is stored.',\n                        default=None)\n    parser.add_argument(\n        '--output_logits_npy',\n        type=str,\n        help=\n        'Numpy file where the generation logits are stored. Use only when num_beams==1',\n        default=None)\n\n    parser.add_argument('--output_log_probs_npy',\n                        type=str,\n                        help='Numpy file where the log_probs are stored',\n                        default=None)\n\n    parser.add_argument('--output_cum_log_probs_npy',\n                        type=str,\n                        help='Numpy file where the cum_log_probs are stored',\n                        default=None)\n\n    parser.add_argument('--tokenizer_dir',\n                        help=\"HF tokenizer config path\",\n                        default='gpt2')\n    parser.add_argument(\n        '--tokenizer_type',\n        help=\n        'Specify that argument when providing a .model file as the tokenizer_dir. '\n        'It allows AutoTokenizer to instantiate the correct tokenizer type.')\n    parser.add_argument('--vocab_file',\n                        help=\"Used for sentencepiece tokenizers\")\n    parser.add_argument('--num_beams',\n                        type=int,\n                        help=\"Use beam search if num_beams > 1\",\n                        default=1)\n    parser.add_argument('--temperature', type=float, default=1.0)\n    parser.add_argument('--top_k', type=int, default=1)\n    parser.add_argument('--top_p', type=float, default=0.0)\n    parser.add_argument('--length_penalty', type=float, default=1.0)\n    parser.add_argument('--repetition_penalty', type=float, default=1.0)\n    parser.add_argument('--presence_penalty', type=float, default=0.0)\n    parser.add_argument('--frequency_penalty', type=float, default=0.0)\n    parser.add_argument('--early_stopping',\n                        type=int,\n                        help='Use early stopping if num_beams > 1'\n                        '1 for early-stopping, 0 for non-early-stopping'\n                        'other values for stopping by length',\n                        default=1)\n    parser.add_argument('--debug_mode',\n                        default=False,\n                        action='store_true',\n                        help=\"Whether or not to turn on the debug mode\")\n    parser.add_argument('--no_add_special_tokens',\n                        dest='add_special_tokens',\n                        default=True,\n                        action='store_false',\n                        help=\"Whether or not to add special tokens\")\n    parser.add_argument('--streaming', default=False, action='store_true')\n    parser.add_argument('--streaming_interval',\n                        type=int,\n                        help=\"How often to return tokens when streaming.\",\n                        default=5)\n    parser.add_argument(\n        '--prompt_table_path',\n        type=str,\n        help=\"Path to .npy file, exported by nemo_prompt_convert.py\")\n    parser.add_argument(\n        '--prompt_tasks',\n        help=\"Comma-separated list of tasks for prompt tuning, e.g., 0,3,1,0\")\n    parser.add_argument('--lora_dir',\n                        type=str,\n                        default=None,\n                        nargs=\"+\",\n                        help=\"The directory of LoRA weights\")\n    parser.add_argument(\n        '--lora_task_uids',\n        type=str,\n        default=None,\n        nargs=\"+\",\n        help=\"The list of LoRA task uids; use -1 to disable the LoRA module\")\n    parser.add_argument('--lora_ckpt_source',\n                        type=str,\n                        default=\"hf\",\n                        choices=[\"hf\", \"nemo\"],\n                        help=\"The source of lora checkpoint.\")\n    parser.add_argument(\n        '--num_prepend_vtokens',\n        nargs=\"+\",\n        type=int,\n        help=\"Number of (default) virtual tokens to prepend to each sentence.\"\n        \" For example, '--num_prepend_vtokens=10' will prepend the tokens\"\n        \" [vocab_size, vocab_size + 1, ..., vocab_size + 9] to the sentence.\")\n    parser.add_argument(\n        '--run_profiling',\n        default=False,\n        action='store_true',\n        help=\"Run several 10 iterations to profile the inference latencies.\")\n    parser.add_argument(\n        '--medusa_choices',\n        type=str,\n        default=None,\n        help=\"Medusa choice to use, if not none, will use Medusa decoding.\"\n        \"   E.g.: [[0, 0, 0, 0], [0, 1, 0], [1, 0], [1, 1]] for 9 medusa tokens.\"\n    )\n\n    return parser.parse_args(args=args)\n\n\ndef parse_input(tokenizer,\n                input_text=None,\n                prompt_template=None,\n                input_file=None,\n                add_special_tokens=True,\n                max_input_length=923,\n                pad_id=None,\n                num_prepend_vtokens=[],\n                model_name=None,\n                model_version=None):\n    if pad_id is None:\n        pad_id = tokenizer.pad_token_id\n\n    batch_input_ids = []\n    if input_file is None:\n        for curr_text in input_text:\n            if prompt_template is not None:\n                curr_text = prompt_template.format(input_text=curr_text)\n            input_ids = tokenizer.encode(curr_text,\n                                         add_special_tokens=add_special_tokens,\n                                         truncation=True,\n                                         max_length=max_input_length)\n            batch_input_ids.append(input_ids)\n    else:\n        if input_file.endswith('.csv'):\n            with open(input_file, 'r') as csv_file:\n                csv_reader = csv.reader(csv_file, delimiter=',')\n                for line in csv_reader:\n                    input_ids = np.array(line, dtype='int32')\n                    batch_input_ids.append(input_ids[-max_input_length:])\n        elif input_file.endswith('.npy'):\n            inputs = np.load(input_file)\n            for row in inputs:\n                input_ids = row[row != pad_id]\n                batch_input_ids.append(input_ids[-max_input_length:])\n        elif input_file.endswith('.txt'):\n            with open(input_file, 'r', encoding='utf-8',\n                      errors='replace') as txt_file:\n                input_text = txt_file.readlines()\n                batch_input_ids = tokenizer(\n                    input_text,\n                    add_special_tokens=add_special_tokens,\n                    truncation=True,\n                    max_length=max_input_length)[\"input_ids\"]\n        else:\n            print('Input file format not supported.')\n            raise SystemExit\n\n    if num_prepend_vtokens:\n        assert len(num_prepend_vtokens) == len(batch_input_ids)\n        base_vocab_size = tokenizer.vocab_size - len(\n            tokenizer.special_tokens_map.get('additional_special_tokens', []))\n        for i, length in enumerate(num_prepend_vtokens):\n            batch_input_ids[i] = list(\n                range(base_vocab_size,\n                      base_vocab_size + length)) + batch_input_ids[i]\n\n    if model_name == 'ChatGLMForCausalLM' and model_version == 'glm':\n        for ids in batch_input_ids:\n            ids.append(tokenizer.sop_token_id)\n\n    batch_input_ids = [\n        torch.tensor(x, dtype=torch.int32) for x in batch_input_ids\n    ]\n    return batch_input_ids\n\n\ndef print_output(tokenizer,\n                 output_ids,\n                 input_lengths,\n                 sequence_lengths,\n                 output_csv=None,\n                 output_npy=None,\n                 context_logits=None,\n                 generation_logits=None,\n                 cum_log_probs=None,\n                 log_probs=None,\n                 output_logits_npy=None,\n                 output_cum_log_probs_npy=None,\n                 output_log_probs_npy=None):\n    batch_size, num_beams, _ = output_ids.size()\n    if output_csv is None and output_npy is None:\n        for batch_idx in range(batch_size):\n            inputs = output_ids[batch_idx][0][:input_lengths[batch_idx]].tolist(\n            )\n            input_text = tokenizer.decode(inputs)\n            # print(f'Input [Text {batch_idx}]: \\\"{input_text}\\\"')\n            for beam in range(num_beams):\n                output_begin = input_lengths[batch_idx]\n                output_end = sequence_lengths[batch_idx][beam]\n                outputs = output_ids[batch_idx][beam][\n                    output_begin:output_end].tolist()\n                output_text = tokenizer.decode(outputs)\n                # print(\n                    # f'Output [Text {batch_idx} Beam {beam}]: \\\"{output_text}\\\"')\n                return output_text\n    return ''\n\n\ndef main(args):\n    runtime_rank = tensorrt_llm.mpi_rank()\n    logger.set_level(args.log_level)\n\n    model_name, model_version = read_model_name(args.engine_dir)\n    if args.tokenizer_dir is None:\n        logger.warning(\n            \"tokenizer_dir is not specified. Try to infer from model_name, but this may be incorrect.\"\n        )\n        args.tokenizer_dir = DEFAULT_HF_MODEL_DIRS[model_name]\n\n    tokenizer, pad_id, end_id = load_tokenizer(\n        tokenizer_dir=args.tokenizer_dir,\n        vocab_file=args.vocab_file,\n        model_name=model_name,\n        model_version=model_version,\n        tokenizer_type=args.tokenizer_type,\n    )\n\n    # # An example to stop generation when the model generate \" London\" on first sentence, \" eventually became\" on second sentence\n    # stop_words_list = [[\" London\"], [\"eventually became\"]]\n    # stop_words_list = tensorrt_llm.runtime.to_word_list_format(stop_words_list, tokenizer)\n    # stop_words_list = torch.Tensor(stop_words_list).to(torch.int32).to(\"cuda\").contiguous()\n    stop_words_list = None\n\n    # # An example to prevent generating \" chef\" on first sentence, \" eventually\" and \" chef before\" on second sentence\n    # bad_words_list = [[\" chef\"], [\" eventually, chef before\"]]\n    # bad_words_list = tensorrt_llm.runtime.to_word_list_format(bad_words_list, tokenizer)\n    # bad_words_list = torch.Tensor(bad_words_list).to(torch.int32).to(\"cuda\").contiguous()\n    bad_words_list = None\n\n    prompt_template = None\n    if args.use_prompt_template and model_name in DEFAULT_PROMPT_TEMPLATES:\n        prompt_template = DEFAULT_PROMPT_TEMPLATES[model_name]\n    if not PYTHON_BINDINGS and not args.use_py_session:\n        logger.warning(\n            \"Python bindings of C++ session is unavailable, fallback to Python session.\"\n        )\n        args.use_py_session = True\n    if args.debug_mode and not args.use_py_session:\n        logger.warning(\n            \"Debug mode is not supported in C++ session for now, fallback to Python session.\"\n        )\n        args.use_py_session = True\n    runner_cls = ModelRunner if args.use_py_session else ModelRunnerCpp\n    runner_kwargs = dict(engine_dir=args.engine_dir,\n                         lora_dir=args.lora_dir,\n                         rank=runtime_rank,\n                         debug_mode=args.debug_mode,\n                         lora_ckpt_source=args.lora_ckpt_source)\n    if args.medusa_choices is not None:\n        args.medusa_choices = ast.literal_eval(args.medusa_choices)\n        assert args.use_py_session, \"Medusa is only supported by py_session\"\n        assert args.temperature == 1.0, \"Medusa should use temperature == 1.0\"\n        assert args.num_beams == 1, \"Medusa should use num_beams == 1\"\n        runner_kwargs.update(medusa_choices=args.medusa_choices)\n    runner = runner_cls.from_dir(**runner_kwargs)\n    \n    \n    \n    \n\n    \n    history = []\n    while True:\n        input_text_with_history = \"\"\n        query = input(\"\\n\u7528\u6237\uff1a\")\n        if query.strip() == \"stop\":\n            break\n        if query.lower() == 'clear':\n            history = []\n            print(\"ChatGLM3-6B: \u5bf9\u8bdd\u5386\u53f2\u5df2\u6e05\u7a7a\")\n            continue\n        history.append(query)\n        for idx, content in enumerate(history):\n            if idx % 2 != 0:\n                input_text_with_history += \"{}\\n\".format(content)\n            else:\n                input_text_with_history += \"<|user|>{}\\n<|assistant|>\".format(content)\n        print(\"\\nChatGLM\uff1a\", end=\"\")\n        batch_input_ids = parse_input(tokenizer=tokenizer,\n                                  input_text=[input_text_with_history],\n                                  prompt_template=prompt_template,\n                                  input_file=args.input_file,\n                                  add_special_tokens=args.add_special_tokens,\n                                  max_input_length=args.max_input_length,\n                                  pad_id=pad_id,\n                                  num_prepend_vtokens=args.num_prepend_vtokens,\n                                  model_name=model_name,\n                                  model_version=model_version)\n        input_lengths = [x.size(0) for x in batch_input_ids]\n        with torch.no_grad():\n            outputs = runner.generate(\n                batch_input_ids,\n                max_new_tokens=args.max_output_len,\n                max_attention_window_size=args.max_attention_window_size,\n                sink_token_length=args.sink_token_length,\n                end_id=end_id,\n                pad_id=pad_id,\n                temperature=args.temperature,\n                top_k=args.top_k,\n                top_p=args.top_p,\n                num_beams=args.num_beams,\n                length_penalty=args.length_penalty,\n                early_stopping=args.early_stopping,\n                repetition_penalty=args.repetition_penalty,\n                presence_penalty=args.presence_penalty,\n                frequency_penalty=args.frequency_penalty,\n                stop_words_list=stop_words_list,\n                bad_words_list=bad_words_list,\n                output_cum_log_probs=(args.output_cum_log_probs_npy != None),\n                output_log_probs=(args.output_log_probs_npy != None),\n                lora_uids=args.lora_task_uids,\n                prompt_table=args.prompt_table_path,\n                prompt_tasks=args.prompt_tasks,\n                streaming=args.streaming,\n                output_sequence_lengths=True,\n                return_dict=True,\n                medusa_choices=args.medusa_choices)\n            torch.cuda.synchronize()\n\n        if args.streaming:\n            for curr_outputs in throttle_generator(outputs,\n                                                args.streaming_interval):\n                if runtime_rank == 0:\n                    output_ids = curr_outputs['output_ids']\n                    sequence_lengths = curr_outputs['sequence_lengths']\n                    cum_log_probs = None\n                    log_probs = None\n                    if args.output_cum_log_probs_npy != None:\n                        cum_log_probs = outputs['cum_log_probs']\n                    if args.output_log_probs_npy != None:\n                        log_probs = outputs['log_probs']\n                    print_output(\n                        tokenizer,\n                        output_ids,\n                        input_lengths,\n                        sequence_lengths,\n                        output_csv=args.output_csv,\n                        output_npy=args.output_npy,\n                        cum_log_probs=cum_log_probs,\n                        log_probs=log_probs,\n                        output_cum_log_probs_npy=args.output_cum_log_probs_npy,\n                        output_log_probs_npy=args.output_log_probs_npy)\n        else:\n            if runtime_rank == 0:\n                output_ids = outputs['output_ids']\n                sequence_lengths = outputs['sequence_lengths']\n                context_logits = None\n                generation_logits = None\n                cum_log_probs = None\n                log_probs = None\n                if runner.gather_context_logits:\n                    context_logits = outputs['context_logits']\n                if runner.gather_generation_logits:\n                    generation_logits = outputs['generation_logits']\n                if args.output_cum_log_probs_npy != None:\n                    cum_log_probs = outputs['cum_log_probs']\n                if args.output_log_probs_npy != None:\n                    log_probs = outputs['log_probs']\n                output_text = print_output(tokenizer,\n                            output_ids,\n                            input_lengths,\n                            sequence_lengths,\n                            output_csv=args.output_csv,\n                            output_npy=args.output_npy,\n                            context_logits=context_logits,\n                            generation_logits=generation_logits,\n                            output_logits_npy=args.output_logits_npy,\n                            cum_log_probs=cum_log_probs,\n                            log_probs=log_probs,\n                            output_cum_log_probs_npy=args.output_cum_log_probs_npy,\n                            output_log_probs_npy=args.output_log_probs_npy)\n                print(output_text)\n                history.append(output_text)\n        # print('\\n\\n\\n\\n')\n        # print(history)\n\n\nif __name__ == '__main__':\n    args = parse_arguments()\n    main(args)\n\n"})})]})}function l(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(u,{...n})}):u(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>a,x:()=>i});var r=t(6540);const o={},s=r.createContext(o);function a(n){const e=r.useContext(s);return r.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function i(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),r.createElement(s.Provider,{value:e},n.children)}}}]);